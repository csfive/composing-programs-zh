# 2.8 æ•ˆç‡

:::info
è¯‘è€…ï¼š[Mancuoj](https://github.com/mancuoj)

æ¥æºï¼š[2.8 Efficiency](http://composingprograms.com/pages/28-efficiency.html)

å¯¹åº”ï¼šæ— 

:::


Decisions of how to represent and process data are often influenced by the efficiency of alternatives. Efficiency refers to the computational resources used by a representation or process, such as how much time and memory are required to compute the result of a function or represent an object. These amounts can vary widely depending on the details of an implementation.

å…³äºå¦‚ä½•è¡¨ç¤ºå’Œå¤„ç†æ•°æ®çš„å†³å®šå¾€å¾€å—åˆ°æ›¿ä»£åŠæ³•æ•ˆç‡çš„å½±å“ã€‚æ•ˆç‡æŒ‡çš„æ˜¯è¡¨ç¤ºæˆ–è¿‡ç¨‹æ‰€ä½¿ç”¨çš„è®¡ç®—èµ„æºï¼Œä¾‹å¦‚è®¡ç®—å‡½æ•°çš„ç»“æœæˆ–è¡¨ç¤ºå¯¹è±¡éœ€è¦å¤šå°‘æ—¶é—´å’Œå†…å­˜ã€‚è¿™äº›é‡å¯ä»¥æ ¹æ®å®æ–½æ–¹å¼çš„ç»†èŠ‚è€Œå¹¿æ³›åœ°å˜åŒ–ã€‚

## 2.8.1  Measuring Efficiency 2.8.1 æµ‹é‡æ•ˆç‡

Measuring exactly how long a program requires to run or how much memory it consumes is challenging, because the results depend upon many details of how a computer is configured. A more reliable way to characterize the efficiency of a program is to measure how many times some event occurs, such as a function call.

ç²¾ç¡®æµ‹é‡ä¸€ä¸ªç¨‹åºéœ€è¦è¿è¡Œå¤šé•¿æ—¶é—´æˆ–æ¶ˆè€—å¤šå°‘å†…å­˜æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç»“æœå–å†³äºè®¡ç®—æœºé…ç½®çš„è®¸å¤šç»†èŠ‚ã€‚æè¿°ç¨‹åºæ•ˆç‡çš„ä¸€ä¸ªæ›´å¯é çš„æ–¹æ³•æ˜¯æµ‹é‡æŸä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¬¡æ•°ï¼Œæ¯”å¦‚å‡½æ•°è°ƒç”¨ã€‚

Let's return to our first tree-recursive function, the `fib` function for computing numbers in the Fibonacci sequence.

è®©æˆ‘ä»¬å›åˆ°ç¬¬ä¸€ä¸ªæ ‘é€’å½’å‡½æ•°ï¼Œ `fib` å‡½æ•°ç”¨äºè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸­çš„æ•°å­—ã€‚

```py
>>> def fib(n):
        if n == 0:
            return 0
        if n == 1:
            return 1
        return fib(n-2) + fib(n-1)
>>> fib(5)
5
```

Consider the pattern of computation that results from evaluating `fib(6)`, depicted below. To compute `fib(5)`, we compute `fib(3)` and `fib(4)`. To compute `fib(3)`, we compute `fib(1)` and `fib(2)`. In general, the evolved process looks like a tree. Each blue dot indicates a completed computation of a Fibonacci number in the traversal of this tree.

è€ƒè™‘è®¡ç®— `fib(6)` æ‰€äº§ç”Ÿçš„è®¡ç®—æ¨¡å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ä¸ºäº†è®¡ç®— `fib(5)` ï¼Œæˆ‘ä»¬è®¡ç®— `fib(3)` å’Œ `fib(4)` ã€‚ä¸ºäº†è®¡ç®— `fib(3)` ï¼Œæˆ‘ä»¬è®¡ç®— `fib(1)` å’Œ `fib(2)` ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ¼”åŒ–è¿‡ç¨‹çœ‹èµ·æ¥åƒä¸€æ£µæ ‘ã€‚æ¯ä¸ªè“ç‚¹è¡¨ç¤ºåœ¨éå†æ­¤æ ‘æ—¶å®Œæˆäº†æ–æ³¢é‚£å¥‘æ•°çš„è®¡ç®—ã€‚

![fib](/sicp-python/fib.png)

This function is instructive as a prototypical tree recursion, but it is a terribly inefficient way to compute Fibonacci numbers because it does so much redundant computation. The entire computation of `fib(3)` is duplicated.

è¿™ä¸ªå‡½æ•°ä½œä¸ºä¸€ä¸ªå…¸å‹çš„æ ‘é€’å½’æ˜¯æœ‰æŒ‡å¯¼æ„ä¹‰çš„ï¼Œä½†æ˜¯å®ƒæ˜¯ä¸€ä¸ªéå¸¸ä½æ•ˆçš„è®¡ç®—æ–æ³¢é‚£å¥‘æ•°çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåšäº†å¤ªå¤šçš„å†—ä½™è®¡ç®—ã€‚é‡å¤ `fib(3)` çš„æ•´ä¸ªè®¡ç®—ã€‚

We can measure this inefficiency. The higher-order `count` function returns an equivalent function to its argument that also maintains a `call_count` attribute. In this way, we can inspect just how many times `fib` is called.

æˆ‘ä»¬å¯ä»¥è¡¡é‡è¿™ç§ä½æ•ˆç‡ã€‚é«˜é˜¶ `count` å‡½æ•°è¿”å›ä¸å…¶å‚æ•°ç­‰æ•ˆçš„å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¹Ÿç»´æŠ¤ `call_count` å±æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ `fib` è¢«è°ƒç”¨äº†å¤šå°‘æ¬¡ã€‚

```py
>>> def count(f):
        def counted(*args):
            counted.call_count += 1
            return f(*args)
        counted.call_count = 0
        return counted
```

By counting the number of calls to `fib`, we see that the calls required grows faster than the Fibonacci numbers themselves. This rapid expansion of calls is characteristic of tree-recursive functions.

é€šè¿‡è®¡ç®—å¯¹ `fib` çš„è°ƒç”¨æ¬¡æ•°ï¼Œæˆ‘ä»¬çœ‹åˆ°æ‰€éœ€çš„è°ƒç”¨æ¬¡æ•°æ¯”æ–æ³¢é‚£å¥‘æ•°åˆ—æœ¬èº«å¢é•¿å¾—æ›´å¿«ã€‚è¿™ç§è°ƒç”¨çš„å¿«é€Ÿæ‰©å±•æ˜¯æ ‘é€’å½’å‡½æ•°çš„ç‰¹å¾ã€‚

```py
>>> fib = count(fib)
>>> fib(19)
4181
>>> fib.call_count
13529
```

**Space.** To understand the space requirements of a function, we must specify generally how memory is used, preserved, and reclaimed in our environment model of computation. In evaluating an expression, the interpreter preserves all *active* environments and all values and frames referenced by those environments. An environment is active if it provides the evaluation context for some expression being evaluated. An environment becomes inactive whenever the function call for which its first frame was created finally returns.

ç©ºé—´ã€‚ä¸ºäº†ç†è§£ä¸€ä¸ªå‡½æ•°çš„ç©ºé—´éœ€æ±‚ï¼Œæˆ‘ä»¬å¿…é¡»è¯¦ç»†è¯´æ˜åœ¨æˆ‘ä»¬çš„è®¡ç®—ç¯å¢ƒæ¨¡å‹ä¸­å†…å­˜æ˜¯å¦‚ä½•ä½¿ç”¨ã€ä¿ç•™å’Œå›æ”¶çš„ã€‚åœ¨è®¡ç®—è¡¨è¾¾å¼æ—¶ï¼Œè§£é‡Šå™¨ä¿ç•™æ‰€æœ‰æ´»åŠ¨ç¯å¢ƒä»¥åŠè¿™äº›ç¯å¢ƒå¼•ç”¨çš„æ‰€æœ‰å€¼å’Œå¸§ã€‚å¦‚æœç¯å¢ƒä¸ºæŸä¸ªæ­£åœ¨è®¡ç®—çš„è¡¨è¾¾å¼æä¾›è®¡ç®—ä¸Šä¸‹æ–‡ï¼Œåˆ™è¯¥ç¯å¢ƒæ˜¯æ´»åŠ¨çš„ã€‚æ¯å½“ä¸ºå…¶åˆ›å»ºç¬¬ä¸€å¸§çš„å‡½æ•°è°ƒç”¨æœ€ç»ˆè¿”å›æ—¶ï¼Œç¯å¢ƒå˜ä¸ºéæ´»åŠ¨çŠ¶æ€ã€‚

For example, when evaluating `fib`, the interpreter proceeds to compute each value in the order shown previously, traversing the structure of the tree. To do so, it only needs to keep track of those nodes that are above the current node in the tree at any point in the computation. The memory used to evaluate the rest of the branches can be reclaimed because it cannot affect future computation. In general, the space required for tree-recursive functions will be proportional to the maximum depth of the tree.

ä¾‹å¦‚ï¼Œå½“è®¡ç®— `fib` æ—¶ï¼Œè§£é‡Šå™¨ç»§ç»­æŒ‰ç…§å‰é¢æ‰€ç¤ºçš„é¡ºåºè®¡ç®—æ¯ä¸ªå€¼ï¼Œéå†æ ‘çš„ç»“æ„ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œå®ƒåªéœ€è¦è·Ÿè¸ªåœ¨è®¡ç®—ä¸­çš„ä»»ä½•ç‚¹ä¸Šæ ‘ä¸­å½“å‰èŠ‚ç‚¹ä¹‹ä¸Šçš„é‚£äº›èŠ‚ç‚¹ã€‚ç”¨äºè®¡ç®—å…¶ä½™åˆ†æ”¯çš„å†…å­˜å¯ä»¥å›æ”¶ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“å°†æ¥çš„è®¡ç®—ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ ‘é€’å½’å‡½æ•°æ‰€éœ€çš„ç©ºé—´ä¸æ ‘çš„æœ€å¤§æ·±åº¦æˆæ­£æ¯”ã€‚

The diagram below depicts the environment created by evaluating `fib(3)`. In the process of evaluating the return expression for the initial application of `fib`, the expression `fib(n-2)` is evaluated, yielding a value of 0. Once this value is computed, the corresponding environment frame (grayed out) is no longer needed: it is not part of an active environment. Thus, a well-designed interpreter can reclaim the memory that was used to store this frame. On the other hand, if the interpreter is currently evaluating `fib(n-1)`, then the environment created by this application of `fib` (in which `n` is 2) is active. In turn, the environment originally created to apply `fib` to 3 is active because its return value has not yet been computed.

ä¸‹å›¾æè¿°äº†é€šè¿‡è¯„ä¼° `fib(3)` åˆ›å»ºçš„ç¯å¢ƒã€‚åœ¨å¯¹ç”¨äº `fib` çš„åˆå§‹åº”ç”¨çš„è¿”å›è¡¨è¾¾å¼æ±‚å€¼çš„å¤„ç†ä¸­ï¼Œè¡¨è¾¾å¼ `fib(n-2)` è¢«æ±‚å€¼ï¼Œäº§ç”Ÿå€¼0ã€‚è®¡ç®—è¯¥å€¼åï¼Œå°†ä¸å†éœ€è¦ç›¸åº”çš„ç¯å¢ƒå¸§ï¼ˆç°æ˜¾ï¼‰ï¼šå®ƒä¸æ˜¯æ´»åŠ¨ç¯å¢ƒä¸€éƒ¨åˆ†ã€‚å› æ­¤ï¼Œè®¾è®¡è‰¯å¥½çš„è§£é‡Šå™¨å¯ä»¥å›æ”¶ç”¨äºå­˜å‚¨æ­¤å¸§çš„å†…å­˜ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœè§£é‡Šç¨‹åºæ­£åœ¨è¯„ä¼° `fib(n-1)` ï¼Œåˆ™ç”± `fib` ï¼ˆå…¶ä¸­ `n` æ˜¯2ï¼‰çš„è¯¥åº”ç”¨åˆ›å»ºçš„ç¯å¢ƒæ˜¯æ´»åŠ¨çš„ã€‚åè¿‡æ¥ï¼Œæœ€åˆä¸ºå°† `fib` åº”ç”¨äº3è€Œåˆ›å»ºçš„ç¯å¢ƒæ˜¯æ´»åŠ¨çš„ï¼Œå› ä¸ºå…¶è¿”å›å€¼è¿˜æ²¡æœ‰è¢«è®¡ç®—ã€‚

The higher-order `count_frames` function tracks `open_count`, the number of calls to the function `f` that have not yet returned. The `max_count` attribute is the maximum value ever attained by `open_count`, and it corresponds to the maximum number of frames that are ever simultaneously active during the course of computation.

é«˜é˜¶ `count_frames` å‡½æ•°è·Ÿè¸ª `open_count` ï¼Œå³å°šæœªè¿”å›çš„å¯¹å‡½æ•° `f` çš„è°ƒç”¨çš„æ•°ç›®ã€‚ `max_count` å±æ€§æ˜¯ `open_count` æ›¾ç»è¾¾åˆ°çš„æœ€å¤§å€¼ï¼Œå¹¶ä¸”å®ƒå¯¹åº”äºåœ¨è®¡ç®—è¿‡ç¨‹æœŸé—´æ›¾ç»åŒæ—¶æ´»åŠ¨çš„å¸§çš„æœ€å¤§æ•°ç›®ã€‚

```py
>>> def count_frames(f):
        def counted(*args):
            counted.open_count += 1
            counted.max_count = max(counted.max_count, counted.open_count)
            result = f(*args)
            counted.open_count -= 1
            return result
        counted.open_count = 0
        counted.max_count = 0
        return counted
>>> fib = count_frames(fib)
>>> fib(19)
4181
>>> fib.open_count
0
>>> fib.max_count
19
>>> fib(24)
46368
>>> fib.max_count
24
```

To summarize, the space requirement of the `fib` function, measured in active frames, is one less than the input, which tends to be small. The time requirement measured in total recursive calls is larger than the output, which tends to be huge.

æ€»è€Œè¨€ä¹‹ï¼Œ `fib` å‡½æ•°çš„ç©ºé—´éœ€æ±‚ï¼ˆåœ¨æ´»åŠ¨å¸§ä¸­æµ‹é‡ï¼‰æ¯”è¾“å…¥å°‘ä¸€ä¸ªï¼Œè¾“å…¥å¾€å¾€å¾ˆå°ã€‚æ€»é€’å½’è°ƒç”¨æ‰€éœ€çš„æ—¶é—´å¤§äºè¾“å‡ºï¼Œåè€…å¾€å¾€å¾ˆå¤§ã€‚

## 2.8.2  Memoization 2.8.2 å›å¿†

Tree-recursive computational processes can often be made more efficient through *memoization*, a powerful technique for increasing the efficiency of recursive functions that repeat computation. A memoized function will store the return value for any arguments it has previously received. A second call to `fib(25)` would not re-compute the return value recursively, but instead return the existing one that has already been constructed.

æ ‘é€’å½’çš„è®¡ç®—è¿‡ç¨‹é€šå¸¸å¯ä»¥é€šè¿‡è®°å¿†æ¥æé«˜æ•ˆç‡ï¼Œè®°å¿†æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜é‡å¤è®¡ç®—çš„é€’å½’å‡½æ•°çš„æ•ˆç‡ã€‚è®°å¿†å‡½æ•°å°†å­˜å‚¨å®ƒä»¥å‰æ¥æ”¶åˆ°çš„ä»»ä½•å‚æ•°çš„è¿”å›å€¼ã€‚å¯¹ `fib(25)` çš„ç¬¬äºŒæ¬¡è°ƒç”¨ä¸ä¼šé€’å½’åœ°é‡æ–°è®¡ç®—è¿”å›å€¼ï¼Œè€Œæ˜¯è¿”å›å·²ç»æ„é€ çš„ç°æœ‰å€¼ã€‚

Memoization can be expressed naturally as a higher-order function, which can also be used as a decorator. The definition below creates a *cache* of previously computed results, indexed by the arguments from which they were computed. The use of a dictionary requires that the argument to the memoized function be immutable.

è®°å¿†åŒ–å¯ä»¥è‡ªç„¶åœ°è¡¨ç¤ºä¸ºé«˜é˜¶å‡½æ•°ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨ä½œè£…é¥°å™¨ã€‚ä¸‹é¢çš„å®šä¹‰åˆ›å»ºäº†ä»¥å‰è®¡ç®—ç»“æœçš„ç¼“å­˜ï¼Œè¿™äº›ç»“æœç”±è®¡ç®—å®ƒä»¬çš„å‚æ•°ç´¢å¼•ã€‚å­—å…¸çš„ä½¿ç”¨è¦æ±‚è®°å¿†å‡½æ•°çš„å‚æ•°æ˜¯ä¸å¯å˜çš„ã€‚

```py
>>> def memo(f):
        cache = {}
        def memoized(n):
            if n not in cache:
                cache[n] = f(n)
            return cache[n]
        return memoized
```

If we apply `memo` to the recursive computation of Fibonacci numbers, a new pattern of computation evolves, depicted below.

å¦‚æœæˆ‘ä»¬å°† `memo` åº”ç”¨äºæ–æ³¢é‚£å¥‘æ•°çš„é€’å½’è®¡ç®—ï¼Œåˆ™ä¼šæ¼”åŒ–å‡ºä¸€ç§æ–°çš„è®¡ç®—æ¨¡å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![fib_memo](/sicp-python/fib_memo.png)

In this computation of `fib(5)`, the results for `fib(2)` and `fib(3)` are reused when computing `fib(4)` on the right branch of the tree. As a result, much of the tree-recursive computation is not required at all.

åœ¨è¯¥ `fib(5)` çš„è®¡ç®—ä¸­ï¼Œå½“åœ¨æ ‘çš„å³åˆ†æ”¯ä¸Šè®¡ç®— `fib(4)` æ—¶ï¼Œé‡ç”¨ `fib(2)` å’Œ `fib(3)` çš„ç»“æœã€‚å› æ­¤ï¼Œæ ¹æœ¬ä¸éœ€è¦å¾ˆå¤šæ ‘é€’å½’è®¡ç®—ã€‚

Using `count`, we can see that the `fib` function is actually only called once for each unique input to `fib`.

ä½¿ç”¨ `count` ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äº `fib` çš„æ¯ä¸ªå”¯ä¸€è¾“å…¥ï¼Œ `fib` å‡½æ•°å®é™…ä¸Šåªè¢«è°ƒç”¨ä¸€æ¬¡ã€‚

```py
>>> counted_fib = count(fib)
>>> fib  = memo(counted_fib)
>>> fib(19)
4181
>>> counted_fib.call_count
20
>>> fib(34)
5702887
>>> counted_fib.call_count
35
```

## 2.8.3  Orders of Growth 2.8.3 ç”Ÿé•¿é¡ºåº

Processes can differ massively in the rates at which they consume the computational resources of space and time, as the previous examples illustrate. However, exactly determining just how much space or time will be used when calling a function is a very difficult task that depends upon many factors. A useful way to analyze a process is to categorize it along with a group of processes that all have similar requirements. A useful categorization is the *order of growth* of a process, which expresses in simple terms how the resource requirements of a process grow as a function of the input.

æ­£å¦‚å‰é¢çš„ç¤ºä¾‹æ‰€ç¤ºï¼Œè¿›ç¨‹æ¶ˆè€—ç©ºé—´å’Œæ—¶é—´è®¡ç®—èµ„æºçš„é€Ÿç‡å¯èƒ½æœ‰å¾ˆå¤§ä¸åŒã€‚ç„¶è€Œï¼Œå‡†ç¡®åœ°ç¡®å®šè°ƒç”¨å‡½æ•°æ—¶å°†ä½¿ç”¨å¤šå°‘ç©ºé—´æˆ–æ—¶é—´æ˜¯ä¸€é¡¹éå¸¸å›°éš¾çš„ä»»åŠ¡ï¼Œå®ƒå–å†³äºè®¸å¤šå› ç´ ã€‚åˆ†ææµç¨‹çš„ä¸€ç§æœ‰ç”¨æ–¹æ³•æ˜¯å°†å…¶ä¸ä¸€ç»„å…·æœ‰ç›¸ä¼¼éœ€æ±‚çš„æµç¨‹æ²¿ç€è¿›è¡Œåˆ†ç±»ã€‚ä¸€ä¸ªæœ‰ç”¨çš„åˆ†ç±»æ˜¯æµç¨‹çš„å¢é•¿é¡ºåºï¼Œå®ƒç”¨ç®€å•çš„æœ¯è¯­è¡¨ç¤ºæµç¨‹çš„èµ„æºéœ€æ±‚å¦‚ä½•ä½œä¸ºè¾“å…¥çš„å‡½æ•°å¢é•¿ã€‚

As an introduction to orders of growth, we will analyze the function `count_factors` below, which counts the number of integers that evenly divide an input `n`. The function attempts to divide `n` by every integer less than or equal to its square root. The implementation takes advantage of the fact that if kï¿½ divides nï¿½ and k<nâˆ’âˆ’âˆšï¿½<ï¿½ , then there is another factor j=n/kï¿½=ï¿½/ï¿½ such that j>nâˆ’âˆ’âˆšï¿½>ï¿½. 

ä½œä¸ºå¯¹å¢é•¿é˜¶æ•°çš„ä»‹ç»ï¼Œæˆ‘ä»¬å°†åˆ†æä¸‹é¢çš„å‡½æ•° `count_factors` ï¼Œè¯¥å‡½æ•°è®¡ç®—å°†è¾“å…¥ `n` æ•´é™¤çš„æ•´æ•°çš„ä¸ªæ•°ã€‚æ­¤å‡½æ•°å°è¯•å°† `n` é™¤ä»¥æ¯ä¸ªå°äºæˆ–ç­‰äºå…¶å¹³æ–¹æ ¹çš„æ•´æ•°ã€‚è¯¥å®ç°åˆ©ç”¨äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³å¦‚æœkğ‘˜æ•´é™¤nğ‘›ä¸”kã€ˆn âˆ’ âˆ’ âˆšğ‘˜ã€ˆğ‘›ï¼Œåˆ™å­˜åœ¨å¦ä¸€ä¸ªå› å­j=n/kğ‘—=ğ‘›/ï¼Œğ‘˜ä½¿å¾—jã€‰n âˆ’ âˆ’ âˆšğ‘—ã€‰ğ‘›ã€‚

How much time is required to evaluate `count_factors`? The exact answer will vary on different machines, but we can make some useful general observations about the amount of computation involved. The total number of times this process executes the body of the `while` statement is the greatest integer less than nâˆ’âˆ’âˆšï¿½. The statements before and after this `while` statement are executed exactly once. So, the total number of statements executed is wâ‹…nâˆ’âˆ’âˆš+vï¿½â‹…ï¿½+ï¿½, where wï¿½ is the number of statements in the `while` body and vï¿½ is the number of statements outside of the `while` statement. Although it isn't exact, this formula generally characterizes how much time will be required to evaluate `count_factors` as a function of the input `n`.

è¯„ä¼° `count_factors` éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿç¡®åˆ‡çš„ç­”æ¡ˆåœ¨ä¸åŒçš„æœºå™¨ä¸Šä¼šæœ‰æ‰€ä¸åŒï¼Œä½†æˆ‘ä»¬å¯ä»¥å¯¹æ‰€æ¶‰åŠçš„è®¡ç®—é‡åšä¸€äº›æœ‰ç”¨çš„ä¸€èˆ¬æ€§è§‚å¯Ÿã€‚è¿™ä¸ªè¿›ç¨‹æ‰§è¡Œ `while` è¯­å¥ä¸»ä½“çš„æ€»æ¬¡æ•°æ˜¯å°äºn âˆ’ âˆ’ âˆšçš„æœ€å¤§æ•´æ•°ğ‘›ã€‚æ­¤ `while` è¯­å¥å‰åçš„è¯­å¥åªæ‰§è¡Œä¸€æ¬¡ã€‚å› æ­¤ï¼Œæ‰§è¡Œçš„è¯­å¥æ€»æ•°ä¸ºwâ‹… n âˆ’ âˆ’ âˆš +vğ‘¤â‹…ğ‘›+ğ‘£ï¼Œå…¶ä¸­wğ‘¤æ˜¯ `while` è¯­å¥ä½“ä¸­çš„è¯­å¥æ•°ï¼Œvğ‘£æ˜¯ `while` è¯­å¥ä¹‹å¤–çš„è¯­å¥æ•°ã€‚å°½ç®¡ä¸ç²¾ç¡®ï¼Œä½†è¯¥å…¬å¼é€šå¸¸è¡¨å¾äº†å°†éœ€è¦å¤šå°‘æ—¶é—´æ¥è¯„ä¼°ä½œä¸ºè¾“å…¥ `n` çš„å‡½æ•°çš„ `count_factors` ã€‚

A more exact description is difficult to obtain. The constants wï¿½ and vï¿½ are not constant at all, because the assignment statements to `factors` are sometimes executed but sometimes not. An order of growth analysis allows us to gloss over such details and instead focus on the general shape of growth. In particular, the order of growth for `count_factors` expresses in precise terms that the amount of time required to compute `count_factors(n)` scales at the rate nâˆ’âˆ’âˆšï¿½, within a margin of some constant factors.

å¾ˆéš¾å¾—åˆ°æ›´ç²¾ç¡®çš„æè¿°ã€‚å¸¸æ•°wğ‘¤å’Œvğ‘£æ ¹æœ¬ä¸æ˜¯å¸¸æ•°ï¼Œå› ä¸ºå¯¹ `factors` çš„èµ‹å€¼è¯­å¥æœ‰æ—¶æ‰§è¡Œï¼Œæœ‰æ—¶ä¸æ‰§è¡Œã€‚ä¸€ä¸ªå¢é•¿åˆ†æçš„é¡ºåºå…è®¸æˆ‘ä»¬æ©ç›–è¿™äº›ç»†èŠ‚ï¼Œè€ŒæŠŠæ³¨æ„åŠ›é›†ä¸­åœ¨å¢é•¿çš„æ€»ä½“å½¢çŠ¶ä¸Šã€‚ç‰¹åˆ«åœ°ï¼Œ `count_factors` çš„å¢é•¿é¡ºåºç”¨ç²¾ç¡®çš„æœ¯è¯­è¡¨ç¤ºäº†è®¡ç®— `count_factors(n)` æ‰€éœ€çš„æ—¶é—´é‡ä»¥n âˆ’ âˆ’ âˆšçš„é€Ÿç‡å¢é•¿ğ‘›ï¼Œåœ¨ä¸€äº›å¸¸æ•°å› å­çš„è£•åº¦å†…ã€‚

**Theta Notation.** Let nï¿½ be a parameter that measures the size of the input to some process, and let R(n)ï¿½(ï¿½) be the amount of some resource that the process requires for an input of size nï¿½. In our previous examples we took nï¿½ to be the number for which a given function is to be computed, but there are other possibilities. For instance, if our goal is to compute an approximation to the square root of a number, we might take nï¿½ to be the number of digits of accuracy required.

Thetaç¬¦å·ã€‚è®¾nğ‘›æ˜¯åº¦é‡æŸä¸ªè¿›ç¨‹çš„è¾“å…¥å¤§å°çš„å‚æ•°ï¼ŒRï¼ˆnï¼‰ğ‘…ï¼ˆğ‘›ï¼‰æ˜¯è¯¥è¿›ç¨‹å¯¹äºå¤§å°ä¸ºnçš„è¾“å…¥æ‰€éœ€çš„æŸä¸ªèµ„æºçš„é‡ğ‘›ã€‚åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å–nğ‘›ä¸ºè¦è®¡ç®—çš„ç»™å®šå‡½æ•°çš„ä¸ªæ•°ï¼Œä½†ä¹Ÿæœ‰å…¶ä»–å¯èƒ½æ€§ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¡ç®—ä¸€ä¸ªæ•°çš„å¹³æ–¹æ ¹çš„è¿‘ä¼¼å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å–nğ‘›ä½œä¸ºæ‰€éœ€ç²¾åº¦çš„ä½æ•°ã€‚

R(n)ï¿½(ï¿½) might measure the amount of memory used, the number of elementary machine steps performed, and so on. In computers that do only a fixed number of steps at a time, the time required to evaluate an expression will be proportional to the number of elementary steps performed in the process of evaluation.
Rï¼ˆnï¼‰ğ‘…ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰å¯ä»¥æµ‹é‡ä½¿ç”¨çš„å†…å­˜é‡ã€æ‰§è¡Œçš„åŸºæœ¬æœºå™¨æ­¥éª¤æ•°ç­‰ã€‚åœ¨ä¸€æ¬¡åªæ‰§è¡Œå›ºå®šæ­¥éª¤æ•°çš„è®¡ç®—æœºä¸­ï¼Œè®¡ç®—è¡¨è¾¾å¼æ‰€éœ€çš„æ—¶é—´å°†ä¸è®¡ç®—è¿‡ç¨‹ä¸­æ‰§è¡Œçš„åŸºæœ¬æ­¥éª¤æ•°æˆæ­£æ¯”ã€‚

We say that R(n)ï¿½(ï¿½) has order of growth Î˜(f(n))Î˜(ï¿½(ï¿½)), written R(n)=Î˜(f(n))ï¿½(ï¿½)=Î˜(ï¿½(ï¿½)) (pronounced "theta of f(n)ï¿½(ï¿½)"), if there are positive constants k1ï¿½1 and k2ï¿½2 independent of nï¿½ such that

æˆ‘ä»¬è¯´Rï¼ˆnï¼‰ğ‘…ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰çš„å¢é•¿é¡ºåºä¸ºÎ˜ï¼ˆfï¼ˆnï¼‰ï¼‰Î˜ï¼ˆğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰ï¼‰ï¼Œè®°ä½œRï¼ˆnï¼‰=Î˜ï¼ˆfï¼ˆnï¼‰ï¼‰ğ‘…ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰= Î˜ï¼ˆğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰ï¼‰ï¼ˆè¯»ä½œâ€œfï¼ˆnï¼‰çš„Î¸ğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰â€ï¼‰ï¼Œå¦‚æœå­˜åœ¨æ­£å¸¸æ•°k 1ğ‘˜1å’Œk 2ğ‘˜2ç‹¬ç«‹äºnğ‘›ä½¿å¾—

k1â‹…f(n)â‰¤R(n)â‰¤k2â‹…f(n)ï¿½1â‹…ï¿½(ï¿½)â‰¤ï¿½(ï¿½)â‰¤ï¿½2â‹…ï¿½(ï¿½)
k 1 â‹… fï¼ˆnï¼‰â‰¤ Rï¼ˆnï¼‰â‰¤ k 2 â‹… fï¼ˆnï¼‰ğ‘˜1 â‹…ğ‘“ï¼‰ï¼ˆğ‘›ï¼‰â‰¤ğ‘…ï¼‰ï¼ˆğ‘›ï¼‰â‰¤ğ‘˜2 â‹…ğ‘“ï¼‰ï¼ˆğ‘›ï¼‰

for any value of nï¿½ larger than some minimum mï¿½. In other words, for large nï¿½, the value R(n)ï¿½(ï¿½) is always sandwiched between two values that both scale with f(n)ï¿½(ï¿½):

å¯¹äºä»»æ„nå€¼ğ‘›å¤§äºæŸä¸ªæœ€å°å€¼mğ‘šã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºè¾ƒå¤§çš„nğ‘›ï¼Œå€¼Rï¼ˆnï¼‰ğ‘…ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰æ€»æ˜¯å¤¹åœ¨ä¸¤ä¸ªéƒ½éšfï¼ˆnï¼‰ç¼©æ”¾çš„å€¼ä¹‹é—´ğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰ï¼š

- A lower bound k1â‹…f(n)ï¿½1â‹…ï¿½(ï¿½) and
- k ~ 1 â‹…fï¼ˆnï¼‰çš„ä¸€ä¸ªä¸‹ç•Œğ‘˜1ğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰åŠ
- An upper bound k2â‹…f(n)ï¿½2â‹…ï¿½(ï¿½)
- k ~ 2 â‹…fï¼ˆnï¼‰çš„ä¸Šç•Œğ‘˜2ğ‘“ï¼ˆè‹±æ–‡ï¼‰ğ‘›ï¼‰

We can apply this definition to show that the number of steps required to evaluate `count_factors(n)` grows as Î˜(nâˆ’âˆ’âˆš)Î˜(ï¿½) by inspecting the function body.

æˆ‘ä»¬å¯ä»¥åº”ç”¨è¿™ä¸ªå®šä¹‰æ¥è¯æ˜è®¡ç®— `count_factors(n)` æ‰€éœ€çš„æ­¥éª¤æ•°éšÎ˜ï¼ˆn âˆ’ âˆ’ âˆšï¼‰Î˜ï¼ˆğ‘›ï¼‰é€šè¿‡æ£€æŸ¥å‡½æ•°ä½“ã€‚

First, we choose k1=1ï¿½1=1 and m=0ï¿½=0, so that the lower bound states that `count_factors(n)` requires at least 1â‹…nâˆ’âˆ’âˆš1â‹…ï¿½ steps for any n>0ï¿½>0. There are at least 4 lines executed outside of the `while` statement, each of which takes at least 1 step to execute. There are at least two lines executed within the `while` body, along with the while header itself. All of these require at least one step. The `while` body is evaluated at least nâˆ’âˆ’âˆšâˆ’1ï¿½âˆ’1 times. Composing these lower bounds, we see that the process requires at least 4+3â‹…(nâˆ’âˆ’âˆšâˆ’1)4+3â‹…(ï¿½âˆ’1) steps, which is always larger than k1â‹…nâˆ’âˆ’âˆšï¿½1â‹…ï¿½.

é¦–å…ˆï¼Œæˆ‘ä»¬é€‰æ‹©k1 =1ğ‘˜1 = 1ä¸”m=0ğ‘š= 0ï¼Œå› æ­¤ä¸‹é™è¡¨æ˜ `count_factors(n)` è‡³å°‘éœ€è¦1â‹… n âˆ’ âˆ’ âˆš 1 â‹…ğ‘›ä»»æ„nã€‰0çš„æ­¥é•¿ğ‘›ã€‰0ã€‚åœ¨ `while` è¯­å¥ä¹‹å¤–è‡³å°‘æ‰§è¡Œäº†4è¡Œï¼Œæ¯ä¸€è¡Œè‡³å°‘éœ€è¦1ä¸ªæ­¥éª¤æ¥æ‰§è¡Œã€‚åœ¨ `while` ä¸»ä½“ä¸­è‡³å°‘æ‰§è¡Œäº†ä¸¤è¡Œä»£ç ï¼Œæ²¿ç€whileå¤´æœ¬èº«ã€‚æ‰€æœ‰è¿™äº›éƒ½éœ€è¦è‡³å°‘ä¸€ä¸ªæ­¥éª¤ã€‚ `while` ä¸»ä½“è‡³å°‘è¯„ä¼°n âˆ’ âˆ’ âˆš âˆ’1ğ‘›-1å€ã€‚æŠŠè¿™äº›ä¸‹ç•Œç»„åˆèµ·æ¥ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™ä¸ªè¿‡ç¨‹è‡³å°‘éœ€è¦4+3â‹…ï¼ˆn âˆ’ âˆ’ âˆš âˆ’1ï¼‰4 + 3 â‹…ï¼ˆğ‘›âˆ’ 1ï¼‰æ­¥æ•°ï¼Œå®ƒæ€»æ˜¯å¤§äºk 1 â‹… n âˆ’ âˆ’ âˆšğ‘˜1ğ‘›ã€‚

Second, we can verify the upper bound. We assume that any single line in the body of `count_factors` requires at most `p` steps. This assumption isn't true for every line of Python, but does hold in this case. Then, evaluating `count_factors(n)` can require at most pâ‹…(5+4nâˆ’âˆ’âˆš)ï¿½â‹…(5+4ï¿½), because there are 5 lines outside of the `while` statement and 4 within (including the header). This upper bound holds even if every `if` header evaluates to true. Finally, if we choose k2=5pï¿½2=5ï¿½, then the steps required is always smaller than k2â‹…nâˆ’âˆ’âˆšï¿½2â‹…ï¿½. Our argument is complete.

ç¬¬äºŒï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯ä¸Šç•Œã€‚æˆ‘ä»¬å‡è®¾ `count_factors` ä¸»ä½“ä¸­çš„ä»»ä½•ä¸€è¡Œæœ€å¤šéœ€è¦ `p` æ­¥ã€‚è¿™ä¸ªå‡è®¾å¹¶ä¸æ˜¯å¯¹Pythonçš„æ¯ä¸€è¡Œéƒ½æˆç«‹ï¼Œä½†åœ¨æœ¬ä¾‹ä¸­æˆç«‹ã€‚é‚£ä¹ˆï¼Œè®¡ç®— `count_factors(n)` æœ€å¤šéœ€è¦pÂ·ï¼ˆ5+4 n âˆ’ âˆ’ âˆšï¼‰ğ‘Â·ï¼ˆ5 + 4ğ‘›ï¼‰ï¼Œå› ä¸º `while` è¯­å¥å¤–é¢æœ‰5è¡Œï¼Œé‡Œé¢æœ‰4è¡Œï¼ˆåŒ…æ‹¬æ ‡é¢˜ï¼‰ã€‚å³ä½¿æ¯ä¸ª `if` æ ‡å¤´çš„å€¼éƒ½ä¸ºtrueï¼Œè¿™ä¸ªä¸Šé™ä¹Ÿæˆç«‹ã€‚æœ€åï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹©k 2 =5pğ‘˜2 = 5ğ‘ï¼Œé‚£ä¹ˆæ‰€éœ€çš„æ­¥æ•°æ€»æ˜¯å°äºk 2 â‹… n âˆ’ âˆ’ âˆšğ‘˜2 â‹…ğ‘›ã€‚æˆ‘ä»¬çš„è®ºç‚¹å·²ç»ç»“æŸäº†ã€‚

## 2.8.4  Example: Exponentiation 2.8.4 ç¤ºä¾‹ï¼šæŒ‡æ•°è¿ç®—

Consider the problem of computing the exponential of a given number. We would like a function that takes as arguments a base `b` and a positive integer exponent `n` and computes bnï¿½ï¿½. One way to do this is via the recursive definition

è€ƒè™‘è®¡ç®—ç»™å®šæ•°çš„æŒ‡æ•°çš„é—®é¢˜ã€‚æˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒçš„å‚æ•°æ˜¯ä¸€ä¸ªåŸºæ•° `b` å’Œä¸€ä¸ªæ­£æ•´æ•°æŒ‡æ•° `n` ï¼Œå¹¶è®¡ç®—b nğ‘ğ‘›ã€‚ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡é€’å½’å®šä¹‰

bnb0=bâ‹…bnâˆ’1=1ï¿½ï¿½=ï¿½â‹…ï¿½ï¿½âˆ’1ï¿½0=1

b nb 0=bb n-1=1ğ‘ğ‘› =ğ‘åœ¨ğ‘ğ‘›ç¬¬1æ­¥ ğ‘ 0 =1

which translates readily into the recursive function

å®ƒå¾ˆå®¹æ˜“è½¬æ¢æˆé€’å½’å‡½æ•°

```py
>>> def exp(b, n):
        if n == 0:
            return 1
        return b * exp(b, n-1)
```

This is a linear recursive process that requires Î˜(n)Î˜(ï¿½) steps and Î˜(n)Î˜(ï¿½) space. Just as with factorial, we can readily formulate an equivalent linear iteration that requires a similar number of steps but constant space.

è¿™æ˜¯ä¸€ä¸ªçº¿æ€§é€’å½’è¿‡ç¨‹ï¼Œéœ€è¦Î˜ï¼ˆnï¼‰Î˜ï¼ˆğ‘›ï¼‰æ­¥å’ŒÎ˜ï¼ˆnï¼‰Î˜ï¼ˆğ‘›ï¼‰ç©ºé—´ã€‚å°±åƒé˜¶ä¹˜ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨å…¬å¼è¡¨ç¤ºä¸€ä¸ªç­‰ä»·çš„çº¿æ€§è¿­ä»£ï¼Œå®ƒéœ€è¦ç›¸ä¼¼çš„æ­¥éª¤æ•°ï¼Œä½†ç©ºé—´ä¸å˜ã€‚

```py
>>> def exp_iter(b, n):
        result = 1
        for _ in range(n):
            result = result * b
        return result
```

We can compute exponentials in fewer steps by using successive squaring. For instance, rather than computing b8ï¿½8 as

é€šè¿‡ä½¿ç”¨è¿ç»­å¹³æ–¹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ›´å°‘çš„æ­¥éª¤è®¡ç®—æŒ‡æ•°ã€‚ä¾‹å¦‚ï¼Œè®¡ç®—Bğ‘æ—¶ï¼Œ

bâ‹…(bâ‹…(bâ‹…(bâ‹…(bâ‹…(bâ‹…(bâ‹…b))))))ï¿½â‹…(ï¿½â‹…(ï¿½â‹…(ï¿½â‹…(ï¿½â‹…(ï¿½â‹…(ï¿½â‹…ï¿½))))))

we can compute it using three multiplications: 

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‰ä¸ªä¹˜æ³•æ¥è®¡ç®—å®ƒï¼š

b2b4b8=bâ‹…b=b2â‹…b2=b4â‹…b4ï¿½2=ï¿½â‹…ï¿½ï¿½4=ï¿½2â‹…ï¿½2ï¿½8=ï¿½4â‹…ï¿½4

This method works fine for exponents that are powers of 2. We can also take advantage of successive squaring in computing exponentials in general if we use the recursive rule

æ­¤æ–¹æ³•é€‚ç”¨äº2çš„å¹‚çš„æŒ‡æ•°ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨é€’å½’è§„åˆ™ï¼Œæˆ‘ä»¬é€šå¸¸ä¹Ÿå¯ä»¥åœ¨è®¡ç®—æŒ‡æ•°æ—¶åˆ©ç”¨è¿ç»­å¹³æ–¹

bn={(b12n)2bâ‹…bnâˆ’1if n is evenif n is oddï¿½ï¿½={(ï¿½12ï¿½)2if ï¿½ is evenï¿½â‹…ï¿½ï¿½âˆ’1if ï¿½ is odd



We can express this method as a recursive function as well: 

æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†æ­¤æ–¹æ³•è¡¨ç¤ºä¸ºé€’å½’å‡½æ•°ï¼š

```py
>>> def square(x):
        return x*x
>>> def fast_exp(b, n):
        if n == 0:
            return 1
        if n % 2 == 0:
            return square(fast_exp(b, n//2))
        else:
            return b * fast_exp(b, n-1)
>>> fast_exp(2, 100)
1267650600228229401496703205376
```

The process evolved by `fast_exp` grows logarithmically with `n` in both space and number of steps. To see this, observe that computing b2nï¿½2ï¿½ using `fast_exp` requires only one more multiplication than computing bnï¿½ï¿½. The size of the exponent we can compute therefore doubles (approximately) with every new multiplication we are allowed. Thus, the number of multiplications required for an exponent of `n` grows about as fast as the logarithm of `n` base 2. The process has Î˜(logn)Î˜(logâ¡ï¿½) growth. The difference between Î˜(logn)Î˜(logâ¡ï¿½) growth and Î˜(n)Î˜(ï¿½) growth becomes striking as nï¿½ becomes large. For example, `fast_exp` for `n` of 1000 requires only 14 multiplications instead of 1000.

ç”± `fast_exp` æ¼”åŒ–çš„è¿‡ç¨‹åœ¨ç©ºé—´å’Œæ­¥éª¤æ•°ä¸Šéƒ½ä¸ `n` æˆå¯¹æ•°å¢é•¿ã€‚è¦äº†è§£è¿™ä¸€ç‚¹ï¼Œè¯·æ³¨æ„ä½¿ç”¨ `fast_exp` è®¡ç®—b 2nğ‘2ğ‘›åªéœ€è¦æ¯”è®¡ç®—b nå¤šä¸€æ¬¡ä¹˜æ³•ğ‘ğ‘›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—çš„æŒ‡æ•°çš„å¤§å°éšç€æˆ‘ä»¬å…è®¸çš„æ¯ä¸€æ¬¡æ–°ä¹˜æ³•è€Œï¼ˆè¿‘ä¼¼åœ°ï¼‰ç¿»å€ã€‚å› æ­¤ï¼Œ `n` çš„æŒ‡æ•°æ‰€éœ€çš„ä¹˜æ³•æ¬¡æ•°å¢é•¿å¾—å¤§çº¦ä¸ä»¥2ä¸ºåº•çš„ `n` çš„å¯¹æ•°ä¸€æ ·å¿«ã€‚è¯¥è¿‡ç¨‹å…·æœ‰Î˜ï¼ˆlognï¼‰Î˜ï¼ˆlogğ‘›ï¼‰å¢é•¿ã€‚å½“nå˜å¤§æ—¶ï¼ŒÎ˜ï¼ˆlognï¼‰Î˜ï¼ˆlogğ‘›ï¼‰å¢é•¿ç‡å’ŒÎ˜ï¼ˆnï¼‰Î˜ï¼ˆğ‘›ï¼‰å¢é•¿ç‡ä¹‹é—´çš„å·®å¼‚å˜å¾—éå¸¸æ˜¾è‘—ğ‘›ã€‚ä¾‹å¦‚ï¼Œå¯¹äº1000çš„ `n` ï¼Œ `fast_exp` ä»…éœ€è¦14æ¬¡ä¹˜æ³•è€Œä¸æ˜¯1000æ¬¡ã€‚

## 2.8.5 å¢é•¿ç±»åˆ«

å¢é•¿çº§åˆ«æ—¨åœ¨ç®€åŒ–è®¡ç®—è¿‡ç¨‹çš„åˆ†æå’Œæ¯”è¾ƒã€‚è®¸å¤šä¸åŒçš„è®¡ç®—è¿‡ç¨‹éƒ½å…·æœ‰ç­‰ä»·çš„å¢é•¿çº§åˆ«ï¼Œè¿™è¡¨æ˜å®ƒä»¬çš„è§„æ¨¡ç›¸ä¼¼ã€‚å¯¹äºè®¡ç®—æœºç§‘å­¦å®¶æ¥è¯´ï¼Œäº†è§£å’Œè¯†åˆ«å¸¸è§çš„å¢é•¿çº§åˆ«ï¼Œå¹¶ç¡®å®šåŒä¸€çº§åˆ«çš„è®¡ç®—è¿‡ç¨‹æ˜¯ä¸€é¡¹å¿…ä¸å¯å°‘çš„æŠ€èƒ½ã€‚

å¸¸æ•°çº§åˆ«ï¼ˆConstantsï¼‰ï¼šå¸¸æ•°é¡¹ä¸å½±å“è¿›ç¨‹çš„å¢é•¿çº§åˆ«ã€‚å› æ­¤ï¼Œ$\Theta(n)$ å’Œ $\Theta(500 \cdot n)$ æ˜¯ç›¸åŒçš„å¢é•¿çº§åˆ«ã€‚è¿™ä¸ªæ€§è´¨æ˜¯ä» $\Theta$ ç¬¦å·è¡¨ç¤ºæ³•çš„å®šä¹‰å¾—å‡ºçš„ï¼Œå®ƒå…è®¸æˆ‘ä»¬é€‰æ‹©ä»»æ„å¸¸æ•° $k1$ å’Œ $k2$ï¼ˆä¾‹å¦‚ $\frac{1}{500}$ ï¼‰ä½œä¸ºä¸Šç•Œå’Œä¸‹ç•Œã€‚ä¸ºç®€å•èµ·è§ï¼Œå¢é•¿çº§åˆ«ä¸­æ€»æ˜¯çœç•¥å¸¸æ•°ã€‚

**Logarithms.** The base of a logarithm does not affect the order of growth of a process. For instance, log2nlog2â¡ï¿½ and log10nlog10â¡ï¿½ are the same order of growth. Changing the base of a logarithm is equivalent to multiplying by a constant factor.

å¯¹æ•°çº§åˆ«ï¼ˆLogarithmsï¼‰ï¼šå¯¹æ•°çš„åº•ä¸å½±å“è¿›ç¨‹çš„å¢é•¿é˜¶ã€‚ä¾‹å¦‚ï¼Œlog 2 n log 2ğ‘›å’Œlog 10 n log 10ğ‘›æ˜¯ç›¸åŒçš„å¢é•¿é¡ºåºã€‚æ›´æ”¹å¯¹æ•°çš„åº•æ•°ç­‰æ•ˆäºä¹˜ä»¥å¸¸æ•°å› å­ã€‚

**Nesting.** When an inner computational process is repeated for each step in an outer process, then the order of growth of the entire process is a product of the number of steps in the outer and inner processes.

ç­‘å·¢ã€‚å½“ä¸€ä¸ªå†…éƒ¨è®¡ç®—è¿‡ç¨‹åœ¨ä¸€ä¸ªå¤–éƒ¨è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥éƒ½è¢«é‡å¤æ—¶ï¼Œé‚£ä¹ˆæ•´ä¸ªè¿‡ç¨‹çš„å¢é•¿é¡ºåºæ˜¯å¤–éƒ¨å’Œå†…éƒ¨è¿‡ç¨‹ä¸­çš„æ­¥éª¤æ•°çš„ä¹˜ç§¯ã€‚

For example, the function `overlap` below computes the number of elements in list `a` that also appear in list `b`.

ä¾‹å¦‚ï¼Œä¸‹é¢çš„å‡½æ•° `overlap` è®¡ç®—åˆ—è¡¨ `a` ä¸­åŒæ—¶å‡ºç°åœ¨åˆ—è¡¨ `b` ä¸­çš„å…ƒç´ æ•°ã€‚

```py
>>> def overlap(a, b):
        count = 0
        for item in a:
            if item in b:
                count += 1
        return count
    
>>> overlap([1, 3, 2, 2, 5, 1], [5, 4, 2])
3
```

The `in` operator for lists requires Î˜(n)Î˜(ï¿½) time, where nï¿½ is the length of the list `b`. It is applied Î˜(m)Î˜(ï¿½) times, where mï¿½ is the length of the list `a`. The `item in b` expression is the inner process, and the `for item in a` loop is the outer process. The total order of growth for this function is Î˜(mâ‹…n)Î˜(ï¿½â‹…ï¿½).

åˆ—è¡¨çš„ `in` è¿ç®—ç¬¦éœ€è¦Î˜ï¼ˆnï¼‰Î˜ï¼ˆğ‘›ï¼‰æ—¶é—´ï¼Œå…¶ä¸­nğ‘›æ˜¯åˆ—è¡¨ `b` çš„é•¿åº¦ã€‚å®ƒè¢«åº”ç”¨Î˜ï¼ˆmï¼‰Î˜ï¼ˆğ‘šï¼‰æ¬¡ï¼Œå…¶ä¸­mğ‘šæ˜¯åˆ—è¡¨ `a` çš„é•¿åº¦ã€‚ `item in b` è¡¨è¾¾å¼æ˜¯å†…éƒ¨è¿›ç¨‹ï¼Œ `for item in a` å¾ªç¯æ˜¯å¤–éƒ¨è¿›ç¨‹ã€‚è¿™ä¸ªå‡½æ•°çš„æ€»å¢é•¿é˜¶ä¸ºÎ˜ï¼ˆmâ‹…nï¼‰Î˜ï¼ˆğ‘šâ‹…ğ‘›ï¼‰ã€‚

**Lower-order terms.** As the input to a process grows, the fastest growing part of a computation dominates the total resources used. Theta notation captures this intuition. In a sum, all but the fastest growing term can be dropped without changing the order of growth.

ä½é˜¶é¡¹ã€‚éšç€è¿›ç¨‹è¾“å…¥çš„å¢é•¿ï¼Œè®¡ç®—ä¸­å¢é•¿æœ€å¿«çš„éƒ¨åˆ†æ”¯é…ç€æ‰€ä½¿ç”¨çš„æ€»èµ„æºã€‚Thetaç¬¦å·è¡¨è¾¾äº†è¿™ç§ç›´è§‰ã€‚æ€»ä¹‹ï¼Œé™¤äº†å¢é•¿æœ€å¿«çš„é¡¹ä¹‹å¤–ï¼Œæ‰€æœ‰é¡¹éƒ½å¯ä»¥å»æ‰ï¼Œè€Œä¸æ”¹å˜å¢é•¿çš„é¡ºåºã€‚

For instance, consider the `one_more` function that returns how many elements of a list `a` are one more than some other element of `a`. That is, in the list `[3, 14, 15, 9]`, the element 15 is one more than 14, so `one_more` will return 1.

ä¾‹å¦‚ï¼Œè€ƒè™‘ `one_more` å‡½æ•°ï¼Œå®ƒè¿”å›åˆ—è¡¨ `a` ä¸­æœ‰å¤šå°‘ä¸ªå…ƒç´ æ¯”åˆ—è¡¨ `a` ä¸­çš„å…¶ä»–å…ƒç´ å¤š1ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨åˆ—è¡¨ `[3, 14, 15, 9]` ä¸­ï¼Œå…ƒç´ 15æ¯”14å¤§1ï¼Œå› æ­¤ `one_more` å°†è¿”å›1ã€‚

```py
>>> def one_more(a):
        return overlap([x-1 for x in a], a)
>>> one_more([3, 14, 15, 9])
1
```

There are two parts to this computation: the list comprehension and the call to `overlap`. For a list `a` of length nï¿½, list comprehension requires Î˜(n)Î˜(ï¿½) steps, while the call to `overlap` requires Î˜(n2)Î˜(ï¿½2) steps. The sum of steps is Î˜(n+n2)Î˜(ï¿½+ï¿½2), but this is not the simplest way of expressing the order of growth.

æ­¤è®¡ç®—æœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼šåˆ—è¡¨è§£æå’Œå¯¹ `overlap` çš„è°ƒç”¨ã€‚å¯¹äºé•¿åº¦ä¸ºnçš„åˆ—è¡¨ `a` ğ‘›ï¼Œåˆ—è¡¨è§£æéœ€è¦Î˜ï¼ˆnï¼‰Î˜ï¼ˆğ‘›ï¼‰æ­¥ï¼Œè€Œå¯¹ `overlap` çš„è°ƒç”¨éœ€è¦Î˜ï¼ˆn 2ï¼‰Î˜ï¼ˆğ‘›2ï¼‰æ­¥ã€‚æ­¥æ•°ä¹‹å’Œä¸ºÎ˜ï¼ˆn+ n 2ï¼‰Î˜ï¼ˆğ‘›+ğ‘›2ï¼‰ï¼Œä½†è¿™å¹¶ä¸æ˜¯è¡¨ç¤ºå¢é•¿é¡ºåºçš„æœ€ç®€å•æ–¹å¼ã€‚

Î˜(n2+kâ‹…n)Î˜(ï¿½2+ï¿½â‹…ï¿½) and Î˜(n2)Î˜(ï¿½2) are equivalent for any constant kï¿½ because the n2ï¿½2 term will eventually dominate the total for any kï¿½. The fact that bounds must hold only for nï¿½ greater than some minimum mï¿½ establishes this equivalence. For simplicity, lower-order terms are always omitted from orders of growth, and so we will never see a sum within a theta expression.

Î˜ï¼ˆn2 +kâ‹…nï¼‰Î˜ï¼ˆğ‘›2 +ğ‘˜â‹…ğ‘›ï¼‰å’ŒÎ˜ï¼ˆn2ï¼‰Î˜ï¼ˆğ‘›2ï¼‰å¯¹ä»»ä½•å¸¸æ•°kéƒ½æ˜¯ç­‰ä»·çš„ï¼Œğ‘˜å› ä¸ºn22ğ‘›é¡¹æœ€ç»ˆä¼šæ”¯é…ä»»ä½•kçš„æ€»å’Œğ‘˜ã€‚è¾¹ç•Œå¿…é¡»ä»…å¯¹ğ‘›å¤§äºæŸä¸ªæœ€å°å€¼mçš„næˆç«‹çš„äº‹å®ğ‘šå»ºç«‹äº†è¿™ç§ç­‰ä»·æ€§ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œä½é˜¶é¡¹æ€»æ˜¯ä»å¢é•¿çš„é˜¶ä¸­çœç•¥ï¼Œå› æ­¤æˆ‘ä»¬æ°¸è¿œä¸ä¼šåœ¨thetaè¡¨è¾¾å¼ä¸­çœ‹åˆ°å’Œã€‚

**Common categories.** Given these equivalence properties, a small set of common categories emerge to describe most computational processes. The most common are listed below from slowest to fastest growth, along with descriptions of the growth as the input increases. Examples for each category follow.

å¸¸è§ç±»åˆ«ï¼šè€ƒè™‘åˆ°è¿™äº›ç­‰ä»·æ€§è´¨ï¼Œå‡ºç°äº†ä¸€å°ç»„å…±åŒçš„èŒƒç•´æ¥æè¿°å¤§å¤šæ•°è®¡ç®—è¿‡ç¨‹ã€‚ä¸‹é¢æŒ‰å¢é•¿é€Ÿåº¦ä»æ…¢åˆ°å¿«çš„é¡ºåºåˆ—å‡ºäº†æœ€å¸¸è§çš„å¢é•¿æ–¹å¼ï¼Œæ²¿ç€æè¿°äº†éšç€æŠ•å…¥çš„å¢åŠ è€Œå¢é•¿çš„æƒ…å†µã€‚æ¯ä¸ªç±»åˆ«çš„ç¤ºä¾‹å¦‚ä¸‹ã€‚

| **ç±»åˆ«** | **ç¬¦å·**          | **å¢é•¿è¯´æ˜**            | **ç¤ºä¾‹**   |
| :------- | :---------------- | :---------------------- | :--------- |
| å¸¸æ•°çº§åˆ« | $\Theta(1)$       | å¢é•¿ä¸è¾“å…¥æ— å…³          | `abs`      |
| å¯¹æ•°çº§åˆ« | $\Theta(\log{n})$ | å€å¢è¾“å…¥ï¼Œå¢åŠ èµ„æº      | `fast_exp` |
| çº¿æ€§çº§åˆ« | $\Theta(n)$       | å¢åŠ è¾“å…¥ï¼Œå¢åŠ èµ„æº      | `exp`      |
| å¹³æ–¹çº§åˆ« | $\Theta(n^{2})$   | å¢åŠ è¾“å…¥ï¼Œå¢åŠ  n ä¸ªèµ„æº | `one_more` |
| æŒ‡æ•°çº§åˆ« | $\Theta(b^{n})$   | å¢åŠ è¾“å…¥ï¼Œå€å¢èµ„æº      | `fib`      |

è¿˜æœ‰ä¸€äº›å…¶ä»–çš„ä¸æ˜¯ç‰¹åˆ«å¸¸è§çš„ç±»åˆ«ï¼Œæ¯”å¦‚ `count_factors` çš„ $\Theta(\sqrt{n})$ å¢é•¿ã€‚

æŒ‡æ•°å¢é•¿ä¸­æè¿°äº†è®¸å¤šä¸åŒçš„å¢é•¿çº§åˆ«ï¼Œå› ä¸ºæ›´æ”¹åŸºæ•° $b$ ä¼šå½±å“å¢é•¿çº§åˆ«ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„æ ‘é€’å½’è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•° `fib` ä¸­ï¼Œå…¶æ­¥æ•°ä¼šéšç€å…¶è¾“å…¥ n å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜ç¬¬ n ä¸ªæ–æ³¢é‚£å¥‘æ•°æ˜¯æœ€æ¥è¿‘ä¸‹åˆ—å¼å­çš„æ•´æ•°

$$
\frac{\phi^{n-2}}{\sqrt{5}}
$$

å…¶ä¸­ $\phi$ æ˜¯é»„é‡‘åˆ†å‰²ç‡ï¼š

$$
\phi=\frac{1+\sqrt{5}}{2} \approx 1.6180
$$

æˆ‘ä»¬è¿˜æŒ‡å‡ºï¼Œæ­¥æ•°ä¸ç»“æœå€¼æˆæ¯”ä¾‹ï¼Œå› æ­¤æ ‘é€’å½’è¿‡ç¨‹éœ€è¦ $\Theta(\phi^{n})$ æ­¥ï¼Œè¿™æ˜¯ä¸€ä¸ªéšç€ n å‘ˆæŒ‡æ•°çº§å¢é•¿çš„å‡½æ•°ã€‚